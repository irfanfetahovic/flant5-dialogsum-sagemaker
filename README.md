# FLAN-T5-Base Dialog Summarization with SageMaker

> ğŸ’¼ **Looking for business overview and ROI?** See the [Client Showcase](docs/FREELANCE_SHOWCASE.md) | [Try Live Demo](#) | [Get a Quote](mailto:your.email@example.com)
>
> ğŸ”§ **For Developers:** [API Documentation](#use-the-production-api-fastapi) | [View on GitHub](#)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10-3.11](https://img.shields.io/badge/python-3.10--3.11-blue.svg)](https://www.python.org/downloads/)
[![Transformers](https://img.shields.io/badge/transformers-4.36.0-orange)](https://huggingface.co/transformers/)

**Technical Documentation** - A production-ready implementation of **FLAN-T5-Base** fine-tuned on Amazon SageMaker for multi-turn dialog summarization using **LoRA/PEFT**. This project demonstrates ML engineering best practices and cloud-native deployment patterns.

## ğŸ¯ Overview

This project demonstrates LLM fine-tuning best practices with:
- **Efficient Training**: LoRA adapter tuning (~6.8M trainable params vs 248M total)
- **Cloud-Native**: Full SageMaker integration with spot instances for cost optimization
- **Production Architecture**: Modular design, configuration management, and error handling
- **Real-world Task**: Abstractive dialog summarization from the DialogSum dataset

### Example

**Input Dialogue:**
```
Tom: Hi Lisa, how are you doing today?
Lisa: Pretty good! I've been working on the new project all morning.
Tom: That's great! How's it going so far?
Lisa: Really well. We've made a lot of progress. The team has been fantastic.
```

**Generated Summary:**
```
Lisa has been working on a new project and has made good progress with a fantastic team.
```

## Project Structure

```
â”œâ”€â”€ config.yaml                    # Configuration file (training hyperparameters)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ .env.example                   # Environment variables template
â”œâ”€â”€ CONTRIBUTING.md                # Contributing guidelines
â”œâ”€â”€ Dockerfile                     # Docker container definition
â”œâ”€â”€ Procfile                       # Heroku deployment configuration
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ deployment.md              # Deployment guide
â”‚   â”œâ”€â”€ FREELANCE_SHOWCASE.md      # Client-facing business case
â”‚   â””â”€â”€ QUICK_REFERENCE.md         # Quick reference guide
â”œâ”€â”€ Makefile                       # Build automation
â”œâ”€â”€ pytest.ini                     # Pytest configuration
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                 # CI/CD workflows
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ demo_app.py                # Streamlit demo application
â”‚   â””â”€â”€ .streamlit/                # Streamlit configuration
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                    # FastAPI application (production)
â”‚   â”œâ”€â”€ models.py                  # Pydantic request/response models
â”‚   â”œâ”€â”€ requirements.txt           # API-specific dependencies
â”‚   â””â”€â”€ README.md                  # API deployment guide
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ sagemaker_training.ipynb   # SageMaker Studio notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sagemaker_config.py        # Reusable SageMaker initialization
â”‚   â”œâ”€â”€ dataset_utils.py           # Dataset loading & formatting
â”‚   â”œâ”€â”€ train.py                   # Training script (SageMaker entry point)
â”‚   â””â”€â”€ inference.py               # Inference functions & utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_dataset.py         # Prepare & upload dataset to S3
â”‚   â”œâ”€â”€ launch_training.py         # Launch SageMaker training job
â”‚   â”œâ”€â”€ deploy_endpoint.py         # Deploy model endpoint
â”‚   â”œâ”€â”€ evaluate.py                # Evaluate model with ROUGE metrics
â”‚   â”œâ”€â”€ example_inference.py       # Example inference usage
â”‚   â””â”€â”€ benchmark.py               # Performance benchmarking
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_dataset_utils.py      # Dataset utilities tests
â”‚   â””â”€â”€ test_inference.py          # Inference functions tests
â””â”€â”€ data/                          # Auto-created during dataset preparation
    â””â”€â”€ jsonl/
        â”œâ”€â”€ train.jsonl
        â””â”€â”€ val.jsonl
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Local Environment                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. Prepare Dataset                                   â”‚   â”‚
â”‚  â”‚    â€¢ Load SAMSum from HuggingFace                    â”‚   â”‚
â”‚  â”‚    â€¢ Convert to JSONL format                        â”‚   â”‚
â”‚  â”‚    â€¢ Upload to S3 (1000 train, 200 val samples)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS SageMaker                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 2. Training Job                                      â”‚   â”‚
â”‚  â”‚    â€¢ Load FLAN-T5-Base (248M params)                â”‚   â”‚
â”‚  â”‚    â€¢ Apply LoRA tuning (6.8M trainable params)      â”‚   â”‚
â”‚  â”‚    â€¢ 3 epochs, batch size 4                         â”‚   â”‚
â”‚  â”‚    â€¢ ml.m5.2xlarge instance (spot)                  â”‚   â”‚
â”‚  â”‚    â€¢ Training time: ~3-4 hours for 1000 samples     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                      â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 3. Model Inference (Real-time or Batch)            â”‚   â”‚
â”‚  â”‚    â€¢ Load fine-tuned FLAN-T5-Base                   â”‚   â”‚
â”‚  â”‚    â€¢ LoRA weights integrated                         â”‚   â”‚
â”‚  â”‚    â€¢ Generate summaries (max 200 tokens)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or 3.11
- AWS account with SageMaker access (only for training)
- AWS credentials configured (`aws configure`) (only for training)
- ~2GB disk space

### Try the Demo (No Training Required!)

You can run the Streamlit demo immediately without any training:

```bash
# Install dependencies
pip install -r requirements.txt

# Run the demo
streamlit run app/demo_app.py
```

The app will use the base FLAN-T5 model (downloads automatically on first run). While not as accurate as a fine-tuned model, it will still generate reasonable summaries.

### Use the Production API (FastAPI)

For production deployments and client integrations, use the FastAPI endpoint:

```bash
# Install API dependencies
pip install fastapi uvicorn pydantic

# Run API server
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

API documentation and interactive testing:
- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc

**Model Loading Options:**
```bash
# Base model (default)
uvicorn api.main:app --port 8000

# With fine-tuned LoRA weights (local)
export PEFT_WEIGHTS_PATH=/path/to/adapter
uvicorn api.main:app --port 8000

# With fine-tuned LoRA weights (S3)
export PEFT_WEIGHTS_PATH=s3://bucket/path
export AWS_ACCESS_KEY_ID=key
export AWS_SECRET_ACCESS_KEY=secret
pip install boto3
uvicorn api.main:app --port 8000
```

See [api/README.md](api/README.md) for deployment options.

### Full Training & Deployment

For production-quality results, follow these steps:

#### 1. Clone & Setup

```bash
git clone https://github.com/yourusername/flan-t5-base-dialogsum-sagemaker.git
cd flan-t5-base-dialogsum-sagemaker

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure AWS credentials
aws configure
```

#### 2. Prepare Dataset

```bash
# Download SAMSum and upload to S3
python scripts/prepare_dataset.py --train-size 1000 --val-size 200
```

This will:
- Download SAMSum dataset from HuggingFace
- Convert to JSONL format
- Upload to your S3 bucket

#### 3. Launch Training

```bash
# Via CLI
python scripts/launch_training.py --job-name my-dialogsum-job --s3-prefix llm

# Or open the notebook in SageMaker Studio
# notebooks/sagemaker_training.ipynb
```

#### 4. Run Inference

```bash
python scripts/example_inference.py \
  --model-id google/flan-t5-base \
  --peft-weights s3://your-bucket/model-artifacts/path

# Output:
# Dialogue: Tom and Lisa discuss project progress...
# Summary: Lisa reports good progress on the new project.
```

#### 5. Evaluate Model

```bash
python scripts/evaluate.py \
  --model-id google/flan-t5-base \
  --peft-weights s3://your-bucket/model-artifacts/path \
  --num-samples 100
```

#### 6. (Optional) Benchmark Performance

```bash
python scripts/benchmark.py \
  --model-id google/flan-t5-base \
  --peft-weights s3://your-bucket/model-artifacts/path \
  --num-samples 50
```

## ğŸ“Š Results & Performance

### Model Efficiency

| Metric | Value |
|--------|-------|
| Base Model Parameters | 248M |
| LoRA Trainable Parameters | 6.8M (2.7%) |
| Training Time | ~3-4 hours (3 epochs, 1000 samples) |
| Instance Type | ml.m5.2xlarge (spot) |
| Estimated Cost | ~$0.40 |
| Dataset | SAMSum (1000 training, 200 validation) |

### Evaluation Metrics (100 test samples)

| Metric | Score |
|--------|-------|
| ROUGE-1 | 0.462 |
| ROUGE-2 | 0.218 |
| ROUGE-L | 0.398 |

**Notes:**
- Metrics computed on validation set from SAMSum (1000 training samples)
- SAMSum dataset produces higher baseline ROUGE scores than DialogSum
- ROUGE scores show good alignment with human-written reference summaries
- Actual performance will be better than small 125-sample baseline

### Example Outputs

**Example 1:**
```
Input:  "Did you see the game last night? It was amazing! 
         The final score was 3-2. What did you think?"

Output: "The game had a final score of 3-2."
```

**Example 2:**
```
Input:  "I've been thinking about getting a new car. 
         I like the Toyota, but I've also heard good things about Honda.
         Do you have any suggestions?"

Output: "The speaker is considering buying either a Toyota or Honda."
```

## ğŸ’¡ Key Features

âœ¨ **Efficient Fine-tuning**
- LoRA adaptation reduces trainable parameters by 97.3%
- Faster training and inference compared to full model tuning
- Lower memory footprint

â˜ï¸ **Cloud-Native Design**
- Full AWS SageMaker integration
- Spot instance support for cost optimization
- S3-hosted datasets
- Automatic model versioning

ğŸ”§ **Well-Structured**
- Modular architecture with separation of concerns
- Basic error handling and logging
- Configuration management via YAML
- Environment variable support for credentials
- CI/CD pipeline for code quality

ğŸ“¦ **Easy Integration**
- Clean API for inference (`summarize_dialogue()`, `batch_summarize()`)
- Support for both local and SageMaker environments
- Pre-built evaluation scripts with ROUGE metrics

## ğŸ§ª Testing & Monitoring

### Run Tests

```bash
pytest tests/ -v

# Output:
# tests/test_dataset_utils.py::test_load_dialogsum_subset PASSED
# tests/test_inference.py::test_summarize_dialogue PASSED
```

### Experiment Tracking with MLflow

Track training experiments locally:

```bash
# Install dependencies
pip install mlflow

# Run training with MLflow tracking
python src/train.py --use-mlflow

# View results in MLflow UI
mlflow ui

# Open browser to http://localhost:5000
```

MLflow tracks:
- Hyperparameters (learning rate, batch size, LoRA config)
- Training metrics (loss, runtime)
- Model parameters (trainable vs total)

### Performance Benchmarking

Measure inference latency and throughput:

```bash
python scripts/benchmark.py --model-id google/flan-t5-base --num-samples 50
```

Benchmark metrics:
- **Latency**: Average, P50, P95 response times
- **Throughput**: Summaries per second
- **Memory**: Model size and RAM usage
- **Token speed**: Tokens generated per second

## ğŸ“ Configuration

Edit `config.yaml` to customize:

```yaml
aws:
  region: us-east-1
  bucket: llm-training-bucket

dataset:
  name: samsung/samsum         # SAMSum dataset (higher quality)
  train_size: 1000             # 1000 training samples (scaled from 125)
  val_size: 200                # 200 validation samples

model:
  name: google/flan-t5-base
  peft:
    r: 8                    # LoRA rank
    lora_alpha: 16          # LoRA scaling
    target_modules: ["q", "v"]
    dropout: 0.05

training:
  instance_type: ml.m5.2xlarge
  epochs: 3
  batch_size: 4               # Increased from 2 for efficiency
  learning_rate: 1e-3
  use_spot: true              # Cost optimization (~70% cheaper)

inference:
  max_new_tokens: 200
  num_beams: 1
```

## ğŸ“š Documentation

### Quick Links
- **[api/README.md](api/README.md)** - FastAPI deployment guide for Heroku, AWS, GCP, Docker
- **[docs/deployment.md](docs/deployment.md)** - Deployment guide and best practices
- **[docs/FREELANCE_SHOWCASE.md](docs/FREELANCE_SHOWCASE.md)** - Business overview & ROI
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Contributing guidelines

### Additional Resources
- [docs/QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) - Quick reference for commands and usage

### Key Dependencies

- `torch==2.1.0` - PyTorch framework
- `transformers==4.36.0` - HuggingFace Transformers
- `datasets==2.16.0` - HuggingFace Datasets (requires `pyarrow>=15.0.0`)
- `peft==0.7.1` - Parameter-Efficient Fine-Tuning
- `sagemaker==2.168.0` - AWS SageMaker SDK

## ğŸ” Environment Variables

Copy `.env.example` to `.env` and fill with your values:

```bash
cp .env.example .env
```

```
SAGEMAKER_ROLE_ARN=arn:aws:iam::YOUR_ACCOUNT_ID:role/SageMakerExecutionRole
AWS_REGION=us-east-1
AWS_BUCKET=llm-training-bucket
```

## ğŸ“– How It Works

### Fine-tuning with LoRA

This project uses **Low-Rank Adaptation (LoRA)** to efficiently fine-tune FLAN-T5-Base:

1. **Load pretrained model** (248M params)
2. **Apply LoRA adapters** to Q and V attention layers
3. **Freeze base model weights** - only train LoRA weights (~6.8M params)
4. **Save lightweight adapters** - adapters are only ~7MB instead of 1GB+

### Inference Pipeline

1. Load base FLAN-T5 model
2. Load LoRA weights from S3
3. Merge weights for inference
4. Generate summary using greedy decoding

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸš¦ Production Readiness

This project is designed as a **learning and demonstration tool**. To deploy in production, you should add:

### Required for Production:
- **Comprehensive Error Handling** - Retry logic, circuit breakers, graceful degradation
- **Model Monitoring** - Track drift, performance degradation, data quality
- **Security Hardening** - AWS Secrets Manager, input validation, authentication
- **Automated Deployment** - Infrastructure as Code (Terraform/CloudFormation)
- **Extensive Testing** - Integration tests, load tests, end-to-end tests
- **Observability** - Structured logging, metrics (CloudWatch/Prometheus), distributed tracing
- **Data Versioning** - DVC or MLflow for dataset and model versioning
- **SLAs & Reliability** - Auto-scaling, health checks, disaster recovery
- **Cost Optimization** - Resource monitoring, budget alerts, spot instance management

### Current State:
âœ… Good code structure and modularity  
âœ… Basic CI/CD for code quality  
âœ… Configuration management  
âœ… Basic logging  
âš ï¸ Limited error handling  
âš ï¸ Minimal test coverage (unit tests only)  
âŒ No model monitoring  
âŒ No production deployment automation  
âŒ No secret management integration  

See [docs/deployment.md](docs/deployment.md) for deployment guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum) - Dialogue summarization dataset
- [FLAN-T5](https://huggingface.co/google/flan-t5-base) - Google's instruction-tuned T5 model
- [PEFT](https://github.com/huggingface/peft) - Parameter-Efficient Fine-Tuning library
- [AWS SageMaker](https://aws.amazon.com/sagemaker/) - Cloud training platform

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue or contact the maintainers.

---

**Built as a learning resource for ML practitioners** ğŸ“

*For production deployments, review the [Production Readiness](#-production-readiness) section above.*
