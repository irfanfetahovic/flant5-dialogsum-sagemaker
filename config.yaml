aws:
  region: us-east-1
  bucket: llm-training-bucket

dataset:
  name: samsung/samsum  # Switched from knkarthick/dialogsum for better quality
  train_size: 1000      # Scaled from 125
  val_size: 200         # Scaled from 32

model:
  name: google/flan-t5-base
  peft:
    r: 8
    lora_alpha: 16
    target_modules: ["q", "v"]
    dropout: 0.05

training:
  instance_type: ml.m5.2xlarge
  instance_count: 1
  epochs: 3
  batch_size: 4         # Increased from 2 for faster training
  learning_rate: 1e-3
  use_spot: true
  max_wait: 3600
  
inference:
  max_new_tokens: 200
  num_beams: 1
