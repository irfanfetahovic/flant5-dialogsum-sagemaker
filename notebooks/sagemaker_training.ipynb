{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7469fcc7",
   "metadata": {},
   "source": [
    "## 1. Initialize SageMaker Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "packages = [\"sagemaker>=2.0\", \"peft\", \"boto3\", \"python-dotenv\", \"PyYAML\"]\n",
    "install_packages(packages)\n",
    "\n",
    "print(\"âœ“ Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfec939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "from sagemaker_config import initialize_sagemaker\n",
    "\n",
    "session, role, bucket, region = initialize_sagemaker()\n",
    "\n",
    "print(f\"âœ“ SageMaker initialized\")\n",
    "print(f\"  Region: {region}\")\n",
    "print(f\"  Bucket: {bucket}\")\n",
    "print(f\"  Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f28a18",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset\n",
    "\n",
    "Run this once to prepare and upload dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d738b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "import os\n",
    "sys.path.insert(0, './')\n",
    "from src.dataset_utils import load_dialogsum_subset, save_jsonl\n",
    "import boto3\n",
    "\n",
    "# Load dataset\n",
    "train_size = 125\n",
    "val_size = 32\n",
    "\n",
    "print(f\"Loading DialogSum (train: {train_size}, val: {val_size})...\")\n",
    "train_data, val_data = load_dialogsum_subset(train_size, val_size)\n",
    "\n",
    "# Create local directory\n",
    "os.makedirs(\"data/jsonl\", exist_ok=True)\n",
    "\n",
    "# Save to JSONL\n",
    "train_path = \"data/jsonl/train.jsonl\"\n",
    "val_path = \"data/jsonl/val.jsonl\"\n",
    "\n",
    "save_jsonl(train_data, train_path)\n",
    "save_jsonl(val_data, val_path)\n",
    "\n",
    "print(f\"âœ“ Saved to {train_path} and {val_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_prefix = \"llm\"\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "s3.upload_file(train_path, bucket, f\"{s3_prefix}/train.jsonl\")\n",
    "s3.upload_file(val_path, bucket, f\"{s3_prefix}/val.jsonl\")\n",
    "\n",
    "print(f\"âœ“ Uploaded to S3:\")\n",
    "print(f\"  s3://{bucket}/{s3_prefix}/train.jsonl\")\n",
    "print(f\"  s3://{bucket}/{s3_prefix}/val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de38fd",
   "metadata": {},
   "source": [
    "## 3. Configure and Launch Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2707d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Display configuration\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Instance: {config['training']['instance_type']}\")\n",
    "print(f\"  Epochs: {config['training']['epochs']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  LoRA rank: {config['model']['peft']['r']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13318d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker estimator\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    instance_type=config[\"training\"][\"instance_type\"],\n",
    "    instance_count=config[\"training\"][\"instance_count\"],\n",
    "    role=role,\n",
    "    transformers_version=\"4.36\",\n",
    "    pytorch_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    use_spot_instances=config[\"training\"][\"use_spot\"],\n",
    "    max_wait=config[\"training\"][\"max_wait\"],\n",
    "    hyperparameters={\n",
    "        \"model-name\": model_name,\n",
    "        \"num-epochs\": config[\"training\"][\"epochs\"],\n",
    "        \"batch-size\": config[\"training\"][\"batch_size\"],\n",
    "        \"learning-rate\": config[\"training\"][\"learning_rate\"],\n",
    "        \"lora-r\": config[\"model\"][\"peft\"][\"r\"],\n",
    "        \"lora-alpha\": config[\"model\"][\"peft\"][\"lora_alpha\"],\n",
    "        \"lora-dropout\": config[\"model\"][\"peft\"][\"dropout\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ“ Estimator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training job\n",
    "print(\"ðŸš€ Launching training job...\")\n",
    "print(f\"Dataset: s3://{bucket}/{s3_prefix}/\")\n",
    "print(\"\")\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\": f\"s3://{bucket}/{s3_prefix}/train.jsonl\",\n",
    "    \"validation\": f\"s3://{bucket}/{s3_prefix}/val.jsonl\"\n",
    "})\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"Job name: {estimator.latest_training_job.name}\")\n",
    "print(f\"Model URI: {estimator.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d7757",
   "metadata": {},
   "source": [
    "## 4. Download and Test Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifacts from S3\n",
    "import shutil\n",
    "\n",
    "model_local_path = \"./model_artifacts\"\n",
    "if os.path.exists(model_local_path):\n",
    "    shutil.rmtree(model_local_path)\n",
    "\n",
    "# Extract S3 path from model_uri (e.g., s3://bucket/path/output/model.tar.gz)\n",
    "model_s3_uri = estimator.model_uri\n",
    "print(f\"Downloading model from: {model_s3_uri}\")\n",
    "\n",
    "# Use sagemaker to extract\n",
    "model_data = session.download_data(\n",
    "    path=model_local_path,\n",
    "    source=model_s3_uri,\n",
    "    target_dir=model_local_path\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model downloaded to: {model_local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae021c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on sample dialogue\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load test data\n",
    "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "test_sample = dataset[\"test\"][200]\n",
    "\n",
    "dialogue = test_sample[\"dialogue\"]\n",
    "human_summary = test_sample[\"summary\"]\n",
    "\n",
    "print(\"Sample Dialogue:\")\n",
    "print(\"-\" * 60)\n",
    "print(dialogue[:300] + \"...\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "print(\"Loading model...\")\n",
    "model_path = model_local_path  # Path where model was downloaded\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, torch_dtype=torch.float32)\n",
    "\n",
    "# Generate summary\n",
    "prompt = f\"Summarize the following conversation:\\n\\n{dialogue}\\n\\nSummary:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=200,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "model_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Human Summary:\")\n",
    "print(human_summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Model Generated Summary:\")\n",
    "print(model_summary)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6595cc",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 10 test samples\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "test_dialogues = dataset[\"test\"][:10][\"dialogue\"]\n",
    "test_summaries = dataset[\"test\"][:10][\"summary\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "print(\"Generating predictions on 10 samples...\")\n",
    "for idx, dialogue in enumerate(test_dialogues):\n",
    "    prompt = f\"Summarize the following conversation:\\n\\n{dialogue}\\n\\nSummary:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=200)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    if (idx + 1) % 5 == 0:\n",
    "        print(f\"  Processed {idx + 1}/10\")\n",
    "\n",
    "# Compute ROUGE\n",
    "results = rouge.compute(\n",
    "    predictions=predictions,\n",
    "    references=test_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ROUGE Evaluation Results (10 test samples)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942686fa",
   "metadata": {},
   "source": [
    "## 6. Optional: Deploy Endpoint\n",
    "\n",
    "Uncomment to deploy a real-time inference endpoint (costs ~$0.05/hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deploy endpoint\n",
    "# print(\"Deploying endpoint...\")\n",
    "# predictor = estimator.deploy(\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "#     initial_instance_count=1,\n",
    "#     endpoint_name=\"flan-t5-dialogsum-endpoint\"\n",
    "# )\n",
    "# print(f\"âœ“ Endpoint deployed: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98869a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test endpoint\n",
    "# test_prompt = \"Summarize: Person A: Hello. Person B: Hi, how are you? Person A: Good, thanks!\"\n",
    "# \n",
    "# result = predictor.predict({\n",
    "#     \"inputs\": test_prompt\n",
    "# })\n",
    "# \n",
    "# print(f\"Endpoint response: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbf8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete endpoint when done (to save costs)\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"âœ“ Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be6aaa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Complete workflow for fine-tuning FLAN-T5-Base:\n",
    "1. âœ“ Initialized SageMaker environment\n",
    "2. âœ“ Prepared and uploaded DialogSum dataset\n",
    "3. âœ“ Configured training with LoRA\n",
    "4. âœ“ Launched remote training job\n",
    "5. âœ“ Downloaded and tested model locally\n",
    "6. âœ“ Evaluated on test set with ROUGE metrics\n",
    "\n",
    "**Total cost**: ~$1-3 USD (using spot instances)\n",
    "**Training time**: 5-15 minutes\n",
    "\n",
    "For production deployment, consider using SageMaker endpoints or batch transform jobs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
