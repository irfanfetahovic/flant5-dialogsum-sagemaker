# .env template (DO NOT COMMIT)
# Copy this to .env and fill with your values

# ========== API Model Configuration ==========
# Base model from HuggingFace (default: google/flan-t5-base)
MODEL_ID=google/flan-t5-base

# (Optional) Path to LoRA adapter weights
# Can be local path: /path/to/adapter
# Or S3 path: s3://your-bucket/path/to/adapter
# PEFT_WEIGHTS_PATH=/path/to/adapter

# (Optional) Model ID used during fine-tuning (defaults to MODEL_ID if not specified)
# PEFT_MODEL_ID=google/flan-t5-base

# AWS Credentials (required if using S3 paths for PEFT_WEIGHTS_PATH)
# AWS_ACCESS_KEY_ID=your_access_key_id
# AWS_SECRET_ACCESS_KEY=your_secret_access_key

# ========== SageMaker Training Configuration ==========
SAGEMAKER_ROLE_ARN=arn:aws:iam::YOUR_ACCOUNT_ID:role/SageMakerExecutionRole
AWS_REGION=us-east-1
AWS_BUCKET=llm-training-bucket
